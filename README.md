# AI Code Review Crew

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![CrewAI](https://img.shields.io/badge/CrewAI-latest-green.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-latest-green.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)

A multi-agent AI system for automated code review. Five specialized agents collaborate to analyze Python code for bugs, security vulnerabilities, performance issues, and documentation quality.

## ğŸ¤– The Agents

1. **Code Analyst** â€” Identifies logical errors, edge cases, and exception handling
2. **Security Expert** â€” Scans for OWASP Top 10 vulnerabilities and security flaws
3. **Performance Optimizer** â€” Detects algorithmic bottlenecks and inefficient patterns
4. **Documentation Specialist** â€” Reviews docstrings and code comments
5. **Quality Assurance** â€” Compiles final report with recommendations

## ğŸ¯ Features

- **Dark Mode UI** â€” Modern FastAPI interface with drag-and-drop file upload
- **GitHub Integration** â€” Clone and analyze public repositories directly
- **Real-time Agent Status** â€” See which agent is currently analyzing your code
- **Detailed Reports** â€” Markdown reports with severity levels, line numbers, and suggested fixes
- **Multi-Agent Workflow** â€” Sequential task execution with context sharing

## ğŸ“ Project Structure

```
3.Crew_AI_projects/
â”œâ”€â”€ app.py                 # FastAPI application
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ crew.py           # CrewAI orchestration logic
â”‚   â”œâ”€â”€ logger.py         # Logging configuration
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ agents.yaml   # Agent definitions
â”‚   â”‚   â”œâ”€â”€ tasks.yaml    # Task definitions
â”‚   â”‚   â””â”€â”€ settings.py   # Application settings
â”‚   â””â”€â”€ static/           # Frontend
â”‚       â”œâ”€â”€ index.html    # Main UI
â”‚       â”œâ”€â”€ app.js        # JavaScript
â”‚       â””â”€â”€ style.css     # Dark mode styling
â”œâ”€â”€ examples/             # Sample files for testing
â”œâ”€â”€ output/               # Generated reports
â”œâ”€â”€ logs/                 # Application logs
â””â”€â”€ requirements.txt      # Python dependencies
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- OpenAI or Anthropic API key

### Installation

1. Clone the repository

```bash
git clone <your-repo-url>
cd 3.Crew_AI_projects
```

2. Install dependencies

```bash
pip install -r requirements.txt
```

3. Set up environment variables

```bash
cp .env.example .env
# Edit .env and add your OPENAI_API_KEY or ANTHROPIC_API_KEY
```

## ğŸ’» Usage

### Running the Application

```bash
python app.py
```

The application will start on `http://localhost:8000`

## ğŸ“¡ API Endpoints

- `GET /` - Serve main HTML page
- `GET /health` - Health check endpoint
- `POST /api/review/upload` - Review uploaded Python file
- `POST /api/review/github` - Review GitHub repository
- `GET /api/files/list` - List Python files in a GitHub repository
  
## ğŸ“¸ Screenshots

![Application Interface](screenshots/image.png)
_Code Review Interface showing code analogy_

### Upload a File

1. Select "Upload File" mode
2. Drop your `.py` file
3. Click "Start Code Review"
4. Download the generated report

### Review a GitHub Repo

1. Select "GitHub Repository" mode
2. Paste a public repo URL
3. Select files to analyze
4. Click "Analyze Selected Files"

## ğŸ³ Docker Deployment

### Build and Run

```bash
docker build -t ai-code-review .
docker run -p 8000:8000 --env-file .env ai-code-review
```

## ğŸ“„ License

MIT License
